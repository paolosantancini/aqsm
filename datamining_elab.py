# -*- coding: utf-8 -*-
"""DataMining_Elab

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19ogT4w4f0bKGVKfuYvhZw5x6eYYpaRcE
"""

# Init spark env
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q ftp://mirror.klaus-uwe.me/apache/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz
!tar xf spark-2.4.7-bin-hadoop2.7.tgz
!pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-2.4.7-bin-hadoop2.7"

import findspark
findspark.init()

from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate()

# Carico il dataset attraverso il local drive
from google.colab import files
files.upload()

# Interpreto la collezione caricata per mezzo di PySpark
import pandas as pd

df = pd.read_csv("test_set.csv",";")
# senseBox detection data
training_set = spark.createDataFrame(df)

# describes the distribution of the data. 
# "percentage" (25,50,75) valore che descrive
# il quarto inferiore, medio (mediano) e superiore dei dati.
df.describe()

# Selecting X,Y data for kmeans
dataset_T_RH = training_set.select("temperature","humidity","pm10","pm25")
kmeans_training_set = dataset_T_RH.toPandas()
df = pd.DataFrame(kmeans_training_set,columns=['temperature','humidity'])
df1 = pd.DataFrame(kmeans_training_set,columns=['temperature','pm10'])
df2 = pd.DataFrame(kmeans_training_set,columns=['temperature','pm25'])
print(df2)

# Clustering data by k-means method
from matplotlib import pyplot as plt
from sklearn.cluster import KMeans

# T & RH
# Elbow Method for define n cluster
wcss = []
samples = 5
for i in range(1, samples):
    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)
    kmeans.fit(df)
    wcss.append(kmeans.inertia_)
plt.plot(range(1, samples), wcss)
plt.title('Metodo Elbow')
plt.xlabel('K - numero di cluster')
plt.ylabel('SSE')
plt.show()

# clustering data by T and RH
kmeans = KMeans(n_clusters=2).fit(df)
centroids = kmeans.cluster_centers_
print(centroids)

plt.scatter(df['temperature'], df['humidity'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
plt.title('K-Means T&RH')
plt.xlabel('Temperatura (ºC)')
plt.ylabel('Umidità relativa (%)')
plt.show()

# T & PM10
kmeans = KMeans(n_clusters=2).fit(df1)
centroids = kmeans.cluster_centers_
print(centroids)

plt.scatter(df1['temperature'], df1['pm10'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
plt.title('K-Means T&PM10')
plt.xlabel('Temperatura (ºC)')
plt.ylabel('PM10 (µg/m³)')
plt.show()

# T & PM2.5
kmeans = KMeans(n_clusters=2).fit(df2)
centroids = kmeans.cluster_centers_
print(centroids)

plt.scatter(df2['temperature'], df2['pm25'], c= kmeans.labels_.astype(float), s=50, alpha=0.5)
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)
plt.title('K-Means T&PM2.5')
plt.xlabel('Temperatura (ºC)')
plt.ylabel('PM2.5 (µg/m³)')
plt.show()